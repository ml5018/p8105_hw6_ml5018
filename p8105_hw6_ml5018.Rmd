---
title: "p8105_hw6_ml5018"
author: "Luan Mengxiao"
date: 2023-11-17
output: github_document
---

This is a R Markdown document for homework 6.

Load the package to be used for data processing.

```{r}
library(tidyverse)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "right"))
```

# Problem 1

## data import

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv") |>
  janitor::clean_names()
```

## data pre-processing

Create a `city_state` variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. For this problem, limit your analysis those for whom `victim_race` is `white` or `black`. Be sure that `victim_age` is numeric.

```{r}
homicide_df = 
  homicide_df |>
  mutate(city_state = str_c(city, state, sep = ", "),
         solved = ifelse(disposition == "Closed by arrest", 1, 0)) |>
  filter(city_state != "Dallas, TX",
         city_state != "Phoenix, AZ",
         city_state != "Kansas City, MO",
         city_state != "Tulsa, AL") |>
  filter(victim_race %in% c("White", "Black")) |>
  mutate(victim_age = as.numeric(victim_age))
```

## logistic regression for Baltimore

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of `glm` as an R object; apply the `broom::tidy` to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
baltimore_df = 
  homicide_df |>
  filter(city_state == "Baltimore, MD") |>
  select(solved, victim_age, victim_sex, victim_race)

baltimore_logistic = 
  baltimore_df |>
  glm(solved ~ victim_age + victim_sex + victim_race, 
      data = _,
      family = binomial())
baltimore_logistic

save(baltimore_logistic, file = "results/baltimore_logistic.RData")

baltimore_logistic |>
  broom::tidy() |>
  knitr::kable()

# alpha = 0.05
baltimore_logistic |>
  broom::tidy() |>
  filter(term == "victim_sexMale") |>
  mutate(
    OR = exp(estimate),
    OR_CI_lower = exp(estimate - qnorm(0.975) * std.error),
    OR_CI_upper = exp(estimate + qnorm(0.975) * std.error)) |>
    select(term, estimate, OR, OR_CI_lower, OR_CI_upper) |>
    knitr::kable()
```

## logistic regression for all cities

Now run `glm` for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of `purrr::map`, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r}
get_logistic = function(citystate){
  homicide_df |>
    filter(city_state == citystate) |>
    glm(solved ~ victim_age + victim_sex + victim_race, 
      data = _,
      family = binomial()) |>
    broom::tidy() |>
    filter(term == "victim_sexMale") |>
    mutate(
    OR = exp(estimate),
    OR_CI_lower = exp(estimate - qnorm(0.975) * std.error),
    OR_CI_upper = exp(estimate + qnorm(0.975) * std.error)) |>
    select(term, estimate, OR, OR_CI_lower, OR_CI_upper)
}

citystate_name = 
  homicide_df |>
  select(city_state) |>
  unique()
homicide_results = 
  citystate_name |>
  mutate(logistic_models = map(city_state, get_logistic)) |>
  unnest(logistic_models)
homicide_results |> knitr::kable()
```

## plot

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r}
homicide_results =
  homicide_results |>
  arrange(OR)

homicide_results |>
  ggplot(aes(x = fct_reorder(city_state, OR), y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) +
  labs(
    x = "city, state",
    y = "estimated OR and 95% CI",
    title = "Estimated ORs and CIs for Each City"
  ) +
  theme(axis.text.x = element_text(hjust = 1, angle = 60, size = 8))
```

It can be concluded that homicides with a male victim are less likely to be solved compared with those with a female victim, since the estimated odds ratio in most cities are less than 1. Most of the confidence intervals for this odds ratio contain the null value 1 even for those with a estimate higher than 1, indicating that only for those cities whose CI does not contain 1, there is a significant difference in the solved rate of homicides with victims of different sex.

# Problem 2

## data import

For this problem, we’ll use the Central Park weather data similar to data we’ve seen elsewhere.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

## bootstrap

The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. We’ll focus on a simple linear regression with `tmax` as the response with `tmin` and `prcp` as the predictors, and are interested in the distribution of two quantities estimated from these data:

* r̂2
* log(β̂1∗β̂2)

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂2 and log(β̂0∗β̂1). Note: `broom::glance()` is helpful for extracting r̂2 from a fitted regression, and `broom::tidy()` (with some additional wrangling) should help in computing log(β̂1∗β̂2).

## estimates

```{r}
bootstrap_df =
  weather_df |>
  bootstrap(n = 5000) |>
  mutate(models = map(strap, ~lm(tmax ~ tmin + prcp, data = .x)))
  
r_squared_df = 
  bootstrap_df |>
  mutate(results = map(models, broom::glance)) |>
  select(-strap, -models) |>
  unnest(results)

log_b1b2_df = 
  bootstrap_df |>
  mutate(results = map(models, broom::tidy)) |>
  select(-strap, -models) |>
  unnest(results) |>
  select(id = .id, term, estimate) |>
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |>
  mutate(log_b1b2 = log(tmin * prcp))

r_squared_df |>
  summarize(r2_estimate = mean(r.squared))
log_b1b2_df |>
  summarize(log_b1b2_estimate = mean(log_b1b2, na.rm = TRUE))
```

For log(β̂1∗β2), there are `r sum(is.na(log_b1b2_df))` NAs produced during calculation due to the negative value of the base number, and the statistics of the parameter is computed based on the remaining non-NA values.

## plots

```{r}
r_squared_df |>
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = "Distribution of Estimated R Squared"
  )

log_b1b2_df |>
  ggplot(aes(x = log_b1b2)) +
  geom_density() +
  labs(
    title = "Distribution of Estimated log(b1*b2)"
  )
```

It can be concluded from the density plots that the distribution of both of the quantities are left-skewed, with log(b1*b2) much more skewed than r squared. Also, NAs are created during the calculation process due to some negative values in logarithms, which may impact the overall distribution of the latter quantity. Baesd on the plots in hand we can tell that the median of the two quantities are approximately 0.92 and -5, respectively.

## quantiles

```{r}
r_squared_df |>
  summarize(
    r2_CI_lower = quantile(r.squared, 0.025),
    r2_CI_upper = quantile(r.squared, 0.975)
  ) |>
  knitr::kable()

log_b1b2_df |>
  summarize(
    log_b1b2_CI_lower = quantile(log_b1b2, 0.025, na.rm = TRUE),
    log_b1b2_CI_upper = quantile(log_b1b2, 0.975, na.rm = TRUE)
  ) |>
  knitr::kable()
```

# Problem 3

## data import and cleaning

In this problem, you will analyze data gathered to understand the effects of several variables on a child’s birthweight.

Load and clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).

```{r}
bwt_df = 
  read_csv("data/birthweight.csv") |>
  janitor::clean_names() |>
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )

sum(is.na(bwt_df))
```

## regression model

Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values – use `add_predictions` and `add_residuals` in making this plot.

```{r}
full_model = 
  bwt_df |>
  lm(bwt ~ ., data =_)
full_model |>
  broom::tidy() |>
  knitr::kable()

stepwise_model = 
  full_model |>
  MASS::stepAIC(object = _, trace = FALSE, direction = "backward")
stepwise_model |>
  broom::tidy() |>
  knitr::kable()

bwt_df |>
  add_predictions(stepwise_model) |>
  add_residuals(stepwise_model) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
    x = "fitted values(predictions)",
    y = "residuals",
    title = "Residuals versus Fitted Values"
  )
```

Using `stepwiseAIC` function to perform stepwise model selection based on a full model, we can conclude from the results that variables to be included in the model are: `babysex`, `bhead`, `blength`, `delwt`, `fincome`, `gaweeks`, `mheight`, `mrace`, `parity`, `ppwt`, `smoken`.

## model comparison

Compare your model to two others:

* One using length at birth and gestational age as predictors (main effects only)
* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

Make this comparison in terms of the cross-validated prediction error; use `crossv_mc` and functions in `purrr` as appropriate.

```{r}
#model_1 = lm(bwt ~ blength + gaweeks, data = bwt_df)
#model_2 = lm(bwt ~ bhead * blength * babysex, data = bwt_df)

cv_df = 
  crossv_mc(bwt_df, 100) |>
  mutate(
    train = map(train, as.tibble),
    test = map(test, as.tibble)
  )

comparison_df = 
  cv_df |>
  mutate(
    model_0 = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = bwt_df)),
    model_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = bwt_df)),
    model_2 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = bwt_df))
  ) |>
  mutate(
    rmse_0 = map2_dbl(model_0, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_1 = map2_dbl(model_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_2 = map2_dbl(model_2, test, \(mod, df) rmse(model = mod, data = df))
  )

comparison_df |>
  summarize(
    mean_rmse_0 = mean(rmse_0),
    mean_rmse_1 = mean(rmse_1),
    mean_rmse_2 = mean(rmse_2)
  ) |>
  knitr::kable()

comparison_df |>
  select(starts_with("rmse")) |>
  mutate(
    model_0 = rmse_0,
    model_1 = rmse_1,
    model_2 = rmse_2
  ) |>
  pivot_longer(
    model_0:model_2,
    names_to = "model",
    values_to = "rmse"
  ) |>
  ggplot(aes(x = model, y = rmse)) +
  geom_violin(aes(fill = model)) +
  labs(
    title = "rmse of 3 Different Models"
  )
```

It can be seen from the table and plot that the model we construct using `stepwiseAIC` has the smallest rmse among the three candidate models, suggesting that it is the most reasonable one to fit under such circumstance.