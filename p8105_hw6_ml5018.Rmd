---
title: "p8105_hw6_ml5018"
author: "Luan Mengxiao"
date: 2023-11-17
output: github_document
---

This is a R Markdown document for homework 6.

Load the package to be used for data processing.

```{r}
library(tidyverse)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "right"))
```

# Problem 1

## data import

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv") |>
  janitor::clean_names()
```

## data pre-processing

Create a `city_state` variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. For this problem, limit your analysis those for whom `victim_race` is `white` or `black`. Be sure that `victim_age` is numeric.

```{r}
homicide_df = 
  homicide_df |>
  mutate(city_state = str_c(city, state, sep = ", "),
         solved = ifelse(disposition == "Closed by arrest", 1, 0)) |>
  filter(city_state != "Dallas, TX",
         city_state != "Phoenix, AZ",
         city_state != "Kansas City, MO",
         city_state != "Tulsa, AL") |>
  filter(victim_race %in% c("White", "Black")) |>
  mutate(victim_age = as.numeric(victim_age))
```

## logistic regression for Baltimore

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of `glm` as an R object; apply the `broom::tidy` to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
baltimore_df = 
  homicide_df |>
  filter(city_state == "Baltimore, MD") |>
  select(solved, victim_age, victim_sex, victim_race)

baltimore_logistic = 
  baltimore_df |>
  glm(solved ~ victim_age + victim_sex + victim_race, 
      data = _,
      family = binomial())
baltimore_logistic

save(baltimore_logistic, file = "results/baltimore_logistic.RData")

baltimore_logistic |>
  broom::tidy() |>
  knitr::kable()

# alpha = 0.05
baltimore_logistic |>
  broom::tidy() |>
  filter(term == "victim_sexMale") |>
  mutate(
    OR = exp(estimate),
    OR_CI_lower = exp(estimate - qnorm(0.975) * std.error),
    OR_CI_upper = exp(estimate + qnorm(0.975) * std.error)) |>
    select(term, estimate, OR, OR_CI_lower, OR_CI_upper) |>
    knitr::kable()
```

## logistic regression for all cities

Now run `glm` for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of `purrr::map`, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r}
get_logistic = function(citystate){
  homicide_df |>
    filter(city_state == citystate) |>
    glm(solved ~ victim_age + victim_sex + victim_race, 
      data = _,
      family = binomial()) |>
    broom::tidy() |>
    filter(term == "victim_sexMale") |>
    mutate(
    OR = exp(estimate),
    OR_CI_lower = exp(estimate - qnorm(0.975) * std.error),
    OR_CI_upper = exp(estimate + qnorm(0.975) * std.error)) |>
    select(term, estimate, OR, OR_CI_lower, OR_CI_upper)
}

citystate_name = 
  homicide_df |>
  select(city_state) |>
  unique()
homicide_results = 
  citystate_name |>
  mutate(logistic_models = map(city_state, get_logistic)) |>
  unnest(logistic_models)
homicide_results |> knitr::kable()
```

## plot

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r}
homicide_results =
  homicide_results |>
  arrange(OR)

homicide_results |>
  ggplot(aes(x = fct_reorder(city_state, OR), y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) +
  labs(
    x = "city, state",
    y = "estimated OR and 95% CI",
    title = "Estimated ORs and CIs for Each City"
  ) +
  theme(axis.text.x = element_text(hjust = 1, angle = 60, size = 8))
```

It can be concluded that homicides with a male victim are less likely to be solved compared with those with a female victim, since the estimated odds ratio in most cities are less than 1. Most of the confidence intervals for this odds ratio contain the null value 1 even for those with a estimate higher than 1, indicating that only for those cities whose CI does not contain 1, there is a significant difference in the solved rate of homicides with victims of different sex.

# Problem 2

## data import

For this problem, we’ll use the Central Park weather data similar to data we’ve seen elsewhere.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

## bootstrap

The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. We’ll focus on a simple linear regression with `tmax` as the response with `tmin` and `prcp` as the predictors, and are interested in the distribution of two quantities estimated from these data:

* r̂2
* log(β̂1∗β̂2)

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂2 and log(β̂0∗β̂1). Note: `broom::glance()` is helpful for extracting r̂2 from a fitted regression, and `broom::tidy()` (with some additional wrangling) should help in computing log(β̂1∗β̂2).

## estimates

```{r}
bootstrap_df =
  weather_df |>
  bootstrap(n = 5000) |>
  mutate(models = map(strap, ~lm(tmax ~ tmin + prcp, data = .x)))
  
r_squared_df = 
  bootstrap_df |>
  mutate(results = map(models, broom::glance)) |>
  select(-strap, -models) |>
  unnest(results)

log_b1b2_df = 
  bootstrap_df |>
  mutate(results = map(models, broom::tidy)) |>
  select(-strap, -models) |>
  unnest(results) |>
  select(id = .id, term, estimate) |>
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |>
  mutate(log_b1b2 = log(tmin * prcp))

r_squared_df |>
  summarize(r2_estimate = mean(r.squared))
log_b1b2_df |>
  summarize(log_b1b2_estimate = mean(log_b1b2, na.rm = TRUE))
```

## plots

```{r}
r_squared_df |>
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = "Distribution of Estimated R Squared"
  )

log_b1b2_df |>
  ggplot(aes(x = log_b1b2)) +
  geom_density() +
  labs(
    title = "Distribution of Estimated log(b1*b2)"
  )
```

It can be concluded from the density plots that the distribution of both of the quantities are left-skewed, with log(b1*b2) much more skewed than r squared. Also, NAs are created during the calculation process due to some negative values in logarithms, which may impact the overall distribution of the latter quantity. Baesd on the plots in hand we can tell that the median of the two quantities are approximately 0.92 and -5, respectively.

## quantiles

```{r}
r_squared_df |>
  summarize(
    r2_CI_lower = quantile(r.squared, 0.025),
    r2_CI_upper = quantile(r.squared, 0.975)
  ) |>
  knitr::kable()

log_b1b2_df |>
  summarize(
    log_b1b2_CI_lower = quantile(log_b1b2, 0.025, na.rm = TRUE),
    log_b1b2_CI_upper = quantile(log_b1b2, 0.975, na.rm = TRUE)
  ) |>
  knitr::kable()
```

